# -*- coding: utf-8 -*-
"""01_logistic_regressionipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yW33ECy6M8onE-qcMey-40-JbsY8HksE
"""

import torch
import torchvision
from torchvision.datasets import MNIST
from torchvision import transforms
from torchvision.transforms import ToTensor
from torch.utils.data import random_split

## dataset
train_ds = MNIST(root ='data',
                train=True,
                download=True,
                transform=ToTensor(),
                target_transform=None)

train_ds, validation_ds = random_split(train_ds, [50000, 10000])

test_ds=MNIST(
    root ='data', 
    train=False,
    download=True,
    transform=ToTensor(),
    target_transform=None
    )

print(len(train_ds), len(validation_ds), len(test_ds))

## dataloader
from torch.utils.data import DataLoader
train_dl = DataLoader(dataset = train_ds,
                      batch_size = 128,
                      shuffle = True)

validation_dl = DataLoader(dataset = validation_ds,
                      batch_size = 128,
                      shuffle = False)

test_dl = DataLoader(dataset = test_ds,
                      batch_size = 128,
                      shuffle = False)

# model
from torch import nn
class LogisticRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    input_features = 28 * 28
    hidden_units = 28
    labels = 10
    ##hidden layer
    self.layer = nn.Sequential( nn.Linear(in_features= input_features, out_features = hidden_units),
                               nn.ReLU(),
                               nn.Linear(in_features= hidden_units, out_features = labels),
                               nn.Softmax(dim=1))

  def forward(self, x):
    x = x.reshape(-1, 784) 
    return self.layer(x)

torch.manual_seed(42)
model_0 = LogisticRegressionModel()

model_0

total_val_loss=0
# train
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params = model_0.parameters(), lr = 1e-5)
def acc_fn(labels, preds):
  r = preds.argmax(dim=1)==labels
  s = torch.sum(r)
  return s/len(labels)

epochs = 100

for epoch in range(epochs):
  #training
  train_loss = 0
  model_0.train()
  for batch in train_dl:
    image, labels = batch
    preds = model_0(image)
    loss = loss_fn(preds, labels)
    train_loss+=loss
    loss.backward()
    optimiser.step()
    optimiser.zero_grad()
  
  train_loss /= len(train_dl)

  #validation
  val_loss = 0
  val_acc = 0
  model_0.eval()
  for batch in validation_dl:
    image, labels = batch
    preds = model_0(image)
    val_loss += loss_fn(preds, labels)
    val_acc += acc_fn(labels, preds)

  val_loss /= len(validation_dl)
  val_acc /= len(validation_dl)

  if epoch %10 == 0: 
    print(f"Epoch : {epoch} | Train Loss: {loss} | Validation Loss: {test_loss}")


#testing
model_0.eval()
test_loss=0
for batch in test_dl:
  image, labels = batch
  preds = model_0(image)
  test_loss += loss_fn(preds, labels)
  # test_acc = acc_fn(labels, preds)
  print(f"Test Loss: {test_loss/len(test_dl)}")

