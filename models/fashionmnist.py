# -*- coding: utf-8 -*-
"""FashionMNIST_non_linear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qrttdUkfCr0skgqm1g018TJCD5NnZGxl
"""

import torch
from torchvision.datasets import FashionMNIST
from torchvision.transforms import ToTensor
import torchmetrics

# datasets
train_ds = FashionMNIST(root = 'data',
                        download = True,
                        train = True,
                        transform = ToTensor(),
                        target_transform=None
                        )
test_ds = FashionMNIST(root = 'data',
                        download = True,
                        train = False,
                        transform = ToTensor(),
                        target_transform=None
                        )



len(train_ds), len(test_ds)

class_names = train_ds.classes
class_names

from torch.utils.data import random_split

train_ds, validation_ds = random_split(train_ds, [50000, 10000])
len(validation_ds)

image, label = train_ds[0]
image.shape, label

import matplotlib.pyplot as plt

plt.imshow(image.squeeze(), cmap='gray')
plt.title(class_names[label])
plt.axis(False)

from torch.utils.data import DataLoader 
# dataloader
train_dl = DataLoader(dataset = train_ds,
                      batch_size = 10000,
                      shuffle = True)
validation_dl = DataLoader(dataset = validation_ds,
                      batch_size = 1000,
                      shuffle = False)
test_dl = DataLoader(dataset = test_ds,
                      batch_size = 1000,
                      shuffle = False)
len(train_dl), len(test_dl)

# model
from torch import nn

class FashionMNIST(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer = nn. Sequential(nn.Flatten(),
                                nn.Linear(in_features = 784, out_features=196),
                                nn.Linear(in_features=196, out_features=49),
                                nn.Linear(in_features=49, out_features = 10),
                                nn.Softmax(dim=1))
    

  def forward(self, x):
    return self.layer(x)

# instance
model = FashionMNIST()

model

image, label = train_ds[0]
plt.imshow(image.squeeze(), cmap='gray')
preds = model(image)

preds, label

# loss and optimiser
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params = model.parameters(), lr = 0.01)

image, label = train_ds[0]
image, label

preds = model(image)
preds.argmax()==label

# for batch in validation_dl:
#   images, labels= batch
#   preds = model(images)
  # print(preds.argmax(dim=1))
  # print(labels)
  # print(preds.argmax(dim=1)==labels)
def acc_fn(labels, preds):
  r = preds.argmax(dim=1)==labels
  s = torch.sum(r)
  return s/len(labels)
  # print(s/len(labels))

# training loop

epochs = 1000
for epoch in range(epochs):
  train_loss = 0
  model.train()
  for batch in train_dl:
    images, labels = batch
    preds = model(images)
    loss = loss_fn(preds, labels)
    train_loss +=loss
    optimiser.zero_grad
    loss.backward()
    optimiser.step()

  train_loss /= len(train_dl)

  val_loss = 0
  val_acc = 0
  model.eval()
  with torch.inference_mode():
    for batch in validation_dl:
      images, labels= batch
      preds = model(images)
      val_loss += loss_fn(preds, labels)
      val_acc += acc_fn(labels, preds)
      # val_acc += torchmetrics.functional.accuracy(preds, labels, task="multiclass", num_classes = 10)
    
    val_loss = val_loss / len(validation_dl) 
    val_acc = val_acc / len(validation_dl) 
  
  if epoch % 100 == 0:
    print(f"\nTrain loss: {train_loss:.5f} | Validation loss: {val_loss:.5f} | Validation acc: {val_acc:.5f}")

with torch.inference_mode():
    for batch in test_dl:
      images, labels= batch
      preds = model(images)
      val_loss += loss_fn(preds, labels)
      val_acc += acc_fn(labels, preds)
      # val_acc += torchmetrics.functional.accuracy(preds, labels, task="multiclass", num_classes = 10)
    
    val_loss = val_loss / len(validation_dl) 
    val_acc = val_acc / len(validation_dl) 

    print(f"Test loss: {val_loss:.5f} | Test acc: {val_acc:.5f}")