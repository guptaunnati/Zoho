# -*- coding: utf-8 -*-
"""01_pytorch_workflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B00bykGPDNHIhfb8fhZgsS_Zm0nJtaiV
"""

import torch
from torch import nn


torch.__version__

"""## Linear Regression


*   Statistical model technique
*   analyse and describe: **relation b/w variable**

Variables: 
1. **dependent variables**: response or output variables
2. **independent variables**: explanatory or predictor variables

**Line of best fit** :
minimizing the sum of squared difference b/w the observation of dependent and independent variables

STEPS:
1. Collection of data
2. Plotting data
3. Calculate line of best fit
4. Evaluate model

**Linear Regression Formula**

Y = a + bX

*Y: independent variable; X: dependent variable; a: intercept; b: slope *
"""

# create known parameters
weight=0.7
bias=0.3

#create data
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y = bias + weight * X
print(X[:10], y[:10])

print(len(X), len(y))

"""## Datasets
1. training set
2. validation set
3. test set

**Generalisation** : Ability of ML to perform well on data, it hasnt seen

###Splitting data : training sets and test sets
"""

# create train/test split
train_split = int(0.8 * len(X))
train_X, train_y= X[:train_split], y[:train_split]
test_X, test_y= X[train_split:], y[train_split:]

print(len(train_X), len(train_y), len(test_X), len(test_y))

"""## Building a function to visualize data

def plot_predictions(train_data=train_X, train_labels=train_y, test_data=test_X, test_labels= test_y, predictions=None):
  #plots training data, test data and compares prediction

###on hold

## 2. Build Model
"""

import torch
from torch import nn

#create linear regression model cass, inheriting nn.module : building blocks for graphs / neural networks
class LinearRegressionModel(nn.Module):
  
  # initialisation, layers of network
  def __init__(self):
    super().__init__()

    #initalize model parameters
    self.weights= nn.Parameter(torch.randn(1, 
                                           requires_grad=True, 
                                           dtype=torch.float))
    self.bias= nn.Parameter(torch.randn(1, 
                                        requires_grad=True, 
                                        dtype=torch.float))
    
  #forward function, data is passed throught the network
  def forward(self, X:torch.Tensor) -> torch.Tensor:
    return self.bias + self.weights * X

"""**requires_grad=True**
pytorch will track the gradient of this specific parameter for use with torch.autograd and gradient descent

Default: requires_grad=True

### checking the contents of pytorch module

-model parameters => `.parameters()`
"""

# create a random seed
torch.manual_seed(42)

# create a instance of the model
model_0= LinearRegressionModel()

# checkout the parameters
list(model_0.parameters())

#list named parameters
print(model_0.state_dict())

print(weight, bias)

"""### Predictions"""

with torch.inference_mode():
  y_pred = model_0(test_X)
print(y_pred)

print(test_y)

"""## 3. Train Model

1. loss func
2. optimizer

Loop:
1. training
2. testing
"""

# setup a loss function
loss_fn=nn.L1Loss()

# setup a optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(model_0.parameters(), lr=0.01 ) #learning rate

"""### Training Loop"""

##Testing loop

epochs = 11940

for epoch in range(epochs):
  model_0.train()  #set model to train mode

  #forward pass
  y_pred = model_0(train_X) 

  #loss
  loss=loss_fn(y_pred, train_y)

  #optimizer zero grad
  optimizer.zero_grad()

  #backpropogation
  loss.backward()

  #optimizer step
  optimizer.step()


  ### Testing
  model_0.eval()

  with torch.inference_mode():

    #forward pass
    test_pred = model_0(test_X) 

    #loss
    test_loss=loss_fn(test_pred, test_y)

#list named parameters
print(model_0.state_dict())

with torch.inference_mode():
  y_pred = model_0(test_X)
print(y_pred)