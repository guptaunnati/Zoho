# -*- coding: utf-8 -*-
"""00_pytorch_fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cSKikRBpVCCUPTNG4zSMebEBSlmmKypT
"""

print("Hello World")

print("Starting with Google Colab")

!nvidia-smi

import torch
print(torch.__version__)

"""# Introduction to Tensors

## Creating Tensors

Pytorh tensors are reated using ` torch.Tensor() `-= https://pytorch.org/docs/stable/tensors.html;

"""

#scalar
scalar=torch.tensor(7)
scalar

scalar.item()

scalar.ndim

# get tensor back as python int
scalar.item()

#vector 
vector=torch.tensor([7, 7])
vector

scalar.shape

vector.ndim

vector.shape

#matrix
matrix=torch.tensor([[1, 2],
                    [3, 4]])
matrix

matrix.ndim

matrix.shape

matrix[0]

matrix[1]

matrix[1,1]

#Tensor
tensor=torch.tensor([[1, 2, 3],
                    [4, 5, 7],
                    [9, 0, 8]])
tensor

tensor.ndim

tensor.shape

x=torch.tensor([[1]])
x

x.ndim

x.shape

x.item()

"""## Random tensors

Why Random tensors?
The way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data

`start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers`

https://pytorch.org/docs/stable/generated/torch.rand.html
"""

# create a random tensor of size(3, 4)
random_tensor=torch.rand(3, 4)

random_tensor

random_tensor.ndim

#create a random tensor similar shape to ana image tensor
random_img_size_tensor= torch.rand(size=(224, 224, 3)) #height, width, color(R, B, G)
random_img_size_tensor.shape, random_img_size_tensor.ndim

"""## Zeros and ones"""

#create a tensor of all zeros
zero_tensor=torch.zeros(3, 4)

zero_tensor

zero_tensor=torch.zeros(3, 4, dtype=torch.int32)

zero_tensor

#create a tensor of all ones
ones_tensor=torch.ones(3, 4)
ones_tensor

zero_tensor+ones_tensor

zero_tensor*ones_tensor

#datatype
ones_tensor.dtype

"""## Creating a range of tensors and tensors like"""

#use torch.range()
torch.arange(1, 11)

torch.arange(start=1, end=11)

torch.arange(start=1, end=11, step=3)

torch.arange(1, 11, 5)

#creating tensors like
ten_zeroes=torch.zeros_like(torch.arange(1, 11))

ten_zeroes

"""## Getting information from tensors

Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.

We've seen these before but three of the most common attributes you'll want to find out about tensors are:
* `shape` - what shape is the tensor? (some operations require specific shape rules)
* `dtype` - what datatype are the elements within the tensor stored in?
* `device` - what device is the tensor stored on? (usually GPU or CPU)

Let's create a random tensor and find out details about it.

"""

# Create a tensor
some_tensor = torch.rand(3, 4)

# Find out details about it
print(some_tensor)
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Device tensor is stored on: {some_tensor.device}") # will default to CPU

"""##Manipulating tensors (tensor operations)
In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.

A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.

These operations are often a wonderful dance between:

Addition
Substraction
Multiplication (element-wise)
Division
Matrix multiplication

##Transpose
"""

X=torch.rand((3,4))
X

X.T

X

tensor_1=torch.tensor([1, 2, 3])
tensor_1.shape

#reshape
x=torch.arange(1, 10)
x_reshaped=x.reshape(1, 9)
print(x_reshaped, x_reshaped.shape)
x_reshaped=x.reshape(9, 1)
x_reshaped, x_reshaped.shape

x_reshaped=x.reshape(3, 3)
x_reshaped, x_reshaped.shape

#view
z=x.view(1, 9)
z

z=x.view(3, 3)
z

z[0, 1]=5
z, x

#stack, vstack
#torch.stack(<tensor_var, ...n time>, dim=0 )
x_stacked= torch.stack([x, x, x, x], dim=0)
x_stacked

#stack, hstack
x_stacked= torch.stack([x, x, x, x], dim=1)
x_stacked

x_stacked= torch.stack([x, x, x, x], dim=-2)
x_stacked

x.shape, x.ndim

"""torch.stack(tensor, dim=0, out=None)

**dim=0** vstack
**dim=1** hstack
"""

#squeeze
#torch.squeeze: removes all 1 dimensions from target tensor
#tensor_var.squeeze()
x= torch.zeros(2, 1, 2, 1, 2)
x

x.size()

y=torch.squeeze(x)
y

y.size()

#torch.unsqueeze: adds a 1 dimension to target tensor at a specific dim
#tensor_var.unsqueeze()
x.unsqueeze(dim=0)

x.unsqueeze(dim=5), x.unsqueeze(dim=5).shape

#torch.permute- rearranges the dim of target tensor in speificified order
x= torch.randn((2, 3, 5))
x, x.size()

x= torch.rand((2, 3, 5))
x, x.shape

# print(torch.permute(x, (1, 0, 2)))
x_permute=x.permute(2,0,1)
x_permute, x_permute.size()

x_permute[0, 0, 0]=78643
x[0, 0, 0], x_permute[0, 0, 0]
# x_permute

"""##Indexing (selecting data from tensors)

### Indexing with pytorch is similar to indexing with Numpy 
"""

# create a tensor
import torch

new_tensor= torch.arange(1, 10).reshape(1, 3, 3)
new_tensor, new_tensor.shape

#Lets index on our new tensor
new_tensor[0, 0]

#9
new_tensor[0, 2, 2]

"""" : " to select all of a target dimension

## Pytorch tensors and Numpy

NumPy is a popular Python numeric computing library.
And because of this, Pytorch has functionality to interact with it
"""

# NumPy array to Tensor
import torch
import numpy as np

array=np.arange(1.0, 8.0)
tensor=torch.from_numpy(array)

array,tensor

array.dtype

"""**default array(dtype=float64), tensor=> dtype=float64**"""

#Tensor to numpy

tensor=torch.ones(7)
array= tensor.numpy()
tensor, array

